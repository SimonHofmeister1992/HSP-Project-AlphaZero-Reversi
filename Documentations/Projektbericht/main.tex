% [12pt,a4paper,bibliography=totocnumbered,listof=totocnumbered]{scrartcl}
\documentclass[12pt,a4paper]{article}
\usepackage{mathtools}
\input{includes}

\begin{document}
\input{commands}


% ----------------------------------------------------------------------------------------------------------
% Titelseite
% ----------------------------------------------------------------------------------------------------------
\MyTitlepage{}{
\texttt{simon1.hofmeister@st.oth-regensburg.de}\\
\texttt{nadiia1.matsko@st.oth-regensburg.de}\\
\texttt{monika.silber@st.oth-regensburg.de}\\
\texttt{simon.wasserburger@st.oth-regensburg.de}}
{15.03.\the\year}

\setcounter{page}{1} 
% ----------------------------------------------------------------------------------------------------------
% Inhaltsverzeichnis
% ----------------------------------------------------------------------------------------------------------
\tableofcontents
\pagebreak


% ----------------------------------------------------------------------------------------------------------
% Inhalt
% ----------------------------------------------------------------------------------------------------------
% Abstände Überschrift
\titlespacing{\section}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}
\titlespacing{\subsection}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}
\titlespacing{\subsubsection}{0pt}{12pt plus 4pt minus 2pt}{-6pt plus 2pt minus 2pt}

% Kopfzeile
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
\renewcommand{\subsubsectionmark}[1]{}
\lhead{Kapitel \thesection}
\rhead{\rightmark}

\onehalfspacing
\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theHsection}{\arabic{section}}
\setcounter{section}{0}
\pagenumbering{arabic}
\setcounter{page}{1}

% ----------------------------------------------------------------------------------
% Kapitel: Einleitung
% ----------------------------------------------------------------------------------

\input{chapters/01_einleitung/einleitung.tex} 


\newpage

% ----------------------------------------------------------------------------------
% Kapitel: Related Work/Theorie
% ----------------------------------------------------------------------------------
\section{Theoretische Grundlagen}

\subsection{Monte Carlo Tree Search}
Der MCTS findet als Alternative zum Alpha-Beta-Search Anwendung. Da der Alpha-Beta-Ansatz einen geringen Verzweigungsgrad und eine angemessene Bewertungsfunktion fordert, ist dessen in Anwendung in Brettspielen, die diese Bedingungen nicht erfüllen, ungeeignet. Der MCTS hingegen bewies sich im Umgang mit solchen Situation \cite{Chaslot2008}.
Basierend auf einer Bestensuche und stochastischer Simulation  wählt der MCTS bei jedem Durchlauf den erfolgversprechendsten Knoten als nächsten Spielzug aus. Entsprechend wird ein Baum aufgebaut, in dem ein Knoten einen konkreten Spielzustand widerspiegelt\cite{Chaslot2008}. Dabei werden die drei Schritte Expansion, Simulation und Backpropagation durchgeführt \cite{Chaslot2008}.

Zur Visualisierung dieses Vorgehens dienen Abbildung \ref{fig:mcts_exp_sim}, sowie Abbildung \ref{fig:mcts_backpropagation}. 
Als erster Schritt erfolgt die Auswahl eines Knotens analog des \glqq{}Selection\grqq{}-Schrittes in Abbildung \ref{fig:mcts_exp_sim}.

Falls der aktuelle Spielzustand noch nicht als Knoten existiert, wird der Baum zunächst expandiert \cite{Chaslot2008}. Dies kann in Abbildung \ref{fig:mcts_exp_sim} im Schritt \glqq{}Expand\grqq eingesehen werden und resultiert darin, dass die nächstmöglichen Aktionen als Kinderknoten an den aktuell betrachteten Knoten angehängt werden.

Um nun die beste Aktion zu ermitteln, werden Spiele ausgehend vom aktuellen Zustand bis hin zum Spielende simuliert. Dabei werden valide Spielzüge zufällig ausgewählt \cite{Chaslot2008}, wie \glqq{}Simulation\grqq in Abbildung \ref{fig:mcts_exp_sim} veranschaulicht. An dieser Stelle werden nicht alle vorhandenen Folgepositionen eines Knotens berücksichtigt, sondern lediglich eine davon. Der letzte Knoten repräsentiert den finalen Spielstand.

Hierbei ist jedoch zu beachten, dass eine reine Zufallsauswahl, die impliziert, dass die Selektion aller Möglichkeiten gleich wahrscheinlich ist, in eher primitivem Spielverhalten resultiert. Mithilfe einer Heuristik können daher aussichtsreichere Spielzüge favorisiert werden. Innerhalb eines Playouts durchlaufene Nodes werden schließlich aktualisiert, indem vermerkt wird, dass sie einmal mehr besucht wurden und welches Spielergebnis sich ergeben hat \cite{Chaslot2008}.

Dieser Prozess ist in Abbildung \ref{fig:mcts_backpropagation} dargestellt. in dieser Grafik wurde der Baum um numerische Werte der Form \glqq{}X$|$Y\grqq  ergänzt, wobei X für die Anzahl an Besuchen steht und Y für die Anzahl, wie oft das Spiel ausgehend vom betrachteten Knoten gewonnen wurden. Die Zahl \glqq{}0\grqq signalisiert dabei das Verlieren, die Zahl \glqq{}1\grqq das Gewinnen des simulierten Spiels. Bevor der Simulationsdurchlauf gestartet wird hat der ausgewählte Knoten den Wert \glqq{}1$|$1\grqq{}. Am Ende angekommen, wird der Spielausgang ermittelt, wodurch der entsprechende Knoten zur Veranschaulichung mit dem entsprechenden Wert ergänzt wird (siehe \glqq{}1\grqq{} - \glqq{}1\grqq{} - \glqq{}0\grqq). Daraufhin werden die Resultate unten angefangen bis hin zum Wurzelknoten durch alle involvierten Nodes zurückgeleitet. Der anfangs betrachtete Knoten hat nun den Wert \glqq{}4$|$3\grqq{}, da er dreimal öfter besucht wurde, wobei das Spiel letztendlich zweimal davon gewonnen wurde. Im Wurzelknoten auf oberster Ebene kann entsprechend eine Änderung der Werte von \glqq{}4$|$2\grqq{} auf \glqq{}7$|$4\grqq{} festgestellt werden.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pics/SelectionExpandSimulation.png}	
\caption{Auswahl eines Knotens mit nachfolgender Expansion und Simulation, eigene Abbildung}
	\label{fig:mcts_exp_sim}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pics/Backpropagation.png}
\caption{Simulation und anschließende Backpropagation der entsprechenden Werte, eigene Abbildung}
	\label{fig:mcts_backpropagation}
\end{figure}

Anzumerken ist, dass zwei verschiedene Policies genutzt werden. Für die Erweiterung des Baums wird eine Tree Policy angewendet, die besagt, dass entsprechende Blattknoten an bereits vorhandene, unbesuchte Knoten angefügt werden. Des Weiteren legt die Default Policy die Simulation fest. Hierbei wird in einem nichtterminalen Spielzustand, der gewöhnlich dem neu hinzugefügt Blattknoten entspricht, ein zufälliges Spiel durchlaufen, um ein ein Spielergebnis zu ermitteln \cite{Browne2012}. 

Der MCTS bleibt so lange aktiv, bis er unterbrochen wird, beispielsweise aufgrund von abgelaufener Rechenzeit. Der zu diesem Zeitpunkt als am erfolgreichsten ermittelte Knoten beziehungsweise Spielzug steht daraufhin fest \cite{Browne2012}.

Zu einem Knoten gehören der entsprechende Spielzustand, den er widerspiegelt, der Spielzug aus dem er resultierte, sowie der aus Simulationen resultierende Reward und wie oft er besucht wurde \cite{Browne2012}.

Nachfolgend werden die konkreten Umsetzungsdetails des MCTS in AlphaGo Zero betrachtet, so wie sie von Silver et al. angewendet wurden \cite{Silver2017}.

In AlphaGo Zero wird eine Variante des Upper Confidence Bound (UCB) zur Auswahl der Knoten angewendet. Die konkrete Abwandlung ist der polynomiale UCB applied to Trees (UCT). Dieser errechnet sie wie folgt:

\begin{equation}
UCT = \frac{Q(n_i)}{N(n_i)} cP(n) \frac{\sqrt{N(n)}}{1+N(n_i)}
\end{equation} 
Wobei c eine Konstante darstellt, die das Maß an Exploration festlegt und P(s,a) für die a-priori-Wahrscheinlichkeit für die Auswahl des jeweiligen Spielzugs steht. Letztlich wird stets der Spielzug ausgewählt, der den maximalen UCT-Wert darstellt \cite{Silver2017}.
Für Abbildung \ref{fig:mcts_exp_sim} bedeutet das, dass dies im \glqq{Selection}\grqq{}-Schritt der grau hinterlegte Knoten wäre.

Ferner ist an dieser Stelle anzumerken, dass der UCB den Kompromiss zwischen Exploration und Exploitation widerspiegelt. Der erste Quotient der UCT-Gleichung steht für das Maß der Ausbeutung und lenkt den Algorithmus dahingehend vielversprechende Knoten weiter zu besuchen. Im Gegensatz dazu wird dazu angehalten Bereiche, die noch nicht oft aufgesucht wurden, verstärkt zu untersuchen. Dies wird als Erkundung bezeichnet und durch den letzten Term des UCT abgebildet. Wichtig hierbei ist es ein Gleichgewicht der beiden Komponenten zu finden \cite{Browne2012}.

Die klassische Simulation von zufälligen Spieldurchläufen entfällt im MCTS vollständig, da noch nicht expandierte Knoten an das NN weitergegeben und dort evaluiert wird. Sobald ein solcher Blattknoten erreicht ist, wird dieser dem NN übergeben, woraufhin die a-priori-Wahrscheinlichkeit und die Bewertung des Spielzugs ermittelt werden. Daraufhin ist der Blattknoten expandiert und alle ausgehenden Kanten beziehungsweise Kinderknoten werden mit initialen Werten belegt.
Anschließend erfolgt die Backpropagation, indem in allen durchlaufenen Knoten die Gewinnwahrscheinlichkeit aktualisiert, sowie die Anzahl der Besuche um den Wert Eins inkrementiert wird \cite{Silver2017}. Im Vergleich zu Abbildung \ref{fig:mcts_backpropagation} bedeutet das, dass zur Expansion des jeweiligen Knotens nicht wie bei der Simulation weiter in die Tiefe gegangen wird, um letztendlich die Endewerte \glqq{}1\grqq - \glqq{}1\grqq - \glqq{}0\grqq zu ermitteln, sondern die vom NN berechneten Werte zur Verfügung stehen. Die Backpropagation bleibt hierbei gleich.

Letztlich wird ein konkreter Spielzug ausgehend vom Wurzelknoten selektiert. Dabei wird der Knoten, der am häufigsten besucht wurde als der beste Spielzug aufgefasst. Der ausgewählte Kindknoten wird der neue Wurzelknoten. Der von ihm ausgehend aufgebaute Teilbaum wird beibehalten, während der der restliche Baum verworfen wird \cite{Silver2017}.

Ferner ist anzumerken, dass Alpha(Go)Zero Kenntnis über die Spielregeln hat, die im MCTS abgerufen werden, um Spielzustände abzubilden, die aus Zügen resultieren und um Endzustände bewerten zu können \cite{Silver2017} \cite{SilverHubert2017}. 

\newpage
\subsection{Neuronales Netz}
Dieses Kapitel widmet sich zunächst der grundlegenden Funktionsweise neuronaler Netze (NN) und beinhaltet Erläuterungen zu elementaren Komponenten. Außerdem folgt eine Darlegung des spezifischen Aufbaus und der Besonderheiten des in Alpha(Go)Zero angewendeten Netzes.

\subsubsection{Grundlagen neuronaler Netze}
Ein NN wird durch eine Vielzahl einzelner miteinander verknüpfter Neuronen beschrieben. Dabei erhalten sie Signale von anderen Neuronen als Input, verarbeiten diese und geben sie als Output an andere weiter. Diese Eingangswerte werden als Vektor X = $\{x_{1}, x_{2}, ..., x_{n}\}$ repräsentiert. Um adäquate Ausgangswerte zu erhalten werden die einzelnen Input-Elemente durch die entsprechenden Komponenten im Vektor W = $\{w_{1}, w_{2}, ..., w_{n}\}$ gewichtet. Des Weiteren kann es zu jedem Neuron einen Bias b geben. Zur Verarbeitung der Eingangswerte wird eine Aktivierungsfunktion herangezogen, die Z = $\{w_{1}x_{1}, w_{2}x_{2}, ..., w_{n}x_{n} + b(W^{T}X + b)\}$ als Input erhält. Das sich hieraus ergebende Resultat entspricht \^{y}. Im weiteren Verlauf wird versucht die Diskrepanz zwischen dem tatsächlichen Wert y und dem geschätzten Wert \^{y} durch Anwendung einer Verlustfunktion zu minimieren. Dieser Vorgang wird als \glqq{}Backpropagation\grqq bezeichnet. Dabei werden Fehler von dem letzten bis hin zum ersten layer zurückgeleitet, um alle beteiligten Gewichte in die entsprechende Richtung anzupassen. Dies wird dadurch erreicht, dass man das Minimum der Fehlerfunktion lokalisiert, das durch Annäherung an den Gradienten der Funktion ermittelt werden kann \cite[S. 75-79]{Sewak2019}. 
%TODO allgemein: mathemat. Funktion, die man approximiert

Neuronale Netze bestehen aus verschiedenen Schichten. Dabei existieren stets der input layer, ein oder mehrere hidden layer und der output layer. Während der input layer über genau so viele Neuronen, wie es Eingangswerte gibt, verfügt, weist der output layer so viele Neuronen auf, wie Ausgangsmerkmale vorhanden sind. Die Neuronenanzahl in den versteckten Zwischenschichten kann variieren. Sobald mehr als ein hidden layer vorliegt, spricht man von einem tiefen neuronalen Netz \cite[S. 77]{Sewak2019}. Der Aufbau wird in Abbildung \ref{fig:aufbau_nn} veranschaulicht.

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{pics/Aufbau_NN.png}	
\caption{Exemplarischer Aufbau eines neuronalen Netzes, Abbildung entnommen aus \cite[S. 44]{Kruse2015}}
\label{fig:aufbau_nn}
\end{figure}

Wenn in einer Schicht alle Neuronen mit allen Elementen aus dem vorangegangen, sowie dem nachfolgenden layer verknüpft sind, wird diese als fully connected bezeichnet \cite[S. 79]{Sewak2019}.

Convolutional Neuronal Networks (CNNs) beschreiben tiefe Netze, die für die Bildverarbeitung ausgelegt sind. Dabei wird die Grafik typischerweise dreidimensional dargestellt, mit der Bildhöhe und -breite, sowie den Farbkanälen in der Tiefe. Hierbei kann eine Schicht als convolutional layer eingebunden werden, die entsprechend eine Faltungsoperation anhand eines vorgegebenen Kerns auf das vorliegende Bild anwendet \cite[S. 85]{Sewak2019}.

Hinsichtlich der Fehlerfunktion spricht man von einem \glqq{}L1 loss\grqq{} beziehungsweise einem \glqq{}L2 loss\grqq{}, wenn der absolute respektive der quadratische Fehler berücksichtigt wird \cite[S. 82]{Sewak2019}. An dieser Stelle ist zu beachten, dass man aufgrund von möglichen negativen Differenzen nicht schlichtweg eine Aufsummierung der Fehler vornehmen kann, da sich Zahlen kleiner und größer Null sonst gegenseitig aufheben können. Deshalb muss mindestens der Betrag der absoluten Zahl genutzt werden. Meist verwendet man jedoch die quadratischen Werte, da diese Vorteile gegenüber dem Betrag aufweisen. Zunächst sind die quadratischen Werte stetig differenzierbar, wodurch die Ableitung der Fehlerfunktion vereinfacht wird. Außerdem vorteilhaft ist, dass durch die Quadrierung größere Diskrepanzen stärker gewichtet werden \cite[S. 41]{Kruse2015}.


Beispiele für Verlustfunktionen sind der \textit{mean squared error} (MSE) oder die \textit{cross entropy}.
Der MSE ist definiert durch \cite[S. 101]{Goodfellow2015}: 

\begin{equation}
f(x) = \frac{1}{m} \sum_{i} ( \hat{y}^{(test)} - y^{(test)} ) ^{2}_{i}
\end{equation}

Die Kreuzentropie wird durch folgende Funktion beschrieben \cite[S. 166]{Goodfellow2015}:

\begin{equation}
L(f_\theta (x), y) = -y log f_\theta (x) - (1 - y) log(1 - f_\theta(x))
\end{equation}

Um die Parameter eines NN zu trainieren, wird das Gradientenabstiegsverfahren genutzt. Da der Output des NN und der gewünschte Wert voneinander abweichen, wie es in der Fehlerfunktion berechnet wird, ist es das Ziel aus dieser Verlustfunktion die Richtung abzuleiten, in die die Parameter angepasst werden müssen, um die Differenz zu minimieren. Die Richtung kann festgestellt werden, indem der Gradient $ \nabla $ der Verlustfunktion ermittelt wird. Dieser stellt einen Vektor da, der die partiellen Ableitungen, also die Ableitungen nach unterschiedlichen Funktionsargumenten, der Fehlerfunktion enthält. Der Gradient dient dabei zur Repräsentation des Steigungsverhaltens einer Funktion und zeigt stets in die Richtung des höchsten Anstiegs. Um Fehler zu verringern, werden Gewichte entsprechend in die entgegengesetzte Richtung verändert. Zur Reduktion der Abweichung werden nun Fehler und Gradient berechnet, woraufhin die Anpassung der Gewichte, die Backpropagation, folgt. Zur Fehlerminimierung wird dieser Prozess iterativ durchlaufen. Zur Berechnung des Fehlers des gesamten NN werden die jeweiligen einzelnen Diskrepanzen aufaddiert \cite[S. 58-60]{Kruse2015}. Ein ein ausführliches Beispiel zur Fehlerminimierung und Berechnung der partiellen Ableitungen findet sich in \cite[S. 64-68]{Kruse2015}.
Des Weiteren ist zu berücksichtigen, dass beim Gradientenabstieg grundsätzlich nicht gewährleistet werden kann, dass ein globales Minimum gefunden wird. Lediglich die Annäherung an ein lokales Minimum kann sichergestellt werden \cite[S. 69]{Kruse2015}. Ein weiteres Problem beim vorgestellten Verfahren ist, die Wahl der optimalen Lernrate $ \theta $, die die Schrittweite angibt. Das bedeutet, je kleiner die die Lernrate ist, desto langsamer, in kleineren Schritten, approximiert man das gesuchte Minimum. Dies kann unter Umständen zu viel Zeit in Anspruch nehmen. Wählt man für die Lernrate allerdings einen großen Wert, so nähert man sich dem gewünschten Punkt in entsprechenden großen Schritten an. Auf diese Weise kann es passieren, dass das Minimum stets übersprungen wird \cite[S. 67]{Kruse2015}. 
%TODO Grafik: Man erkennt diesen Prozess...
%TODO optimale Werte für theta

Eine Möglichkeit diesem Problem bei einer gleichzeitig hohen Lernrate entgegenzuwirken liegt in der Inklusion des Momentum. Dabei wird der Wert der vorangegangen Richtung für die Berechnung der neuen Richtung berücksichtigt, wodurch Oszillationen verringert werden und das Minimum schneller erreicht wird \cite{Rumelhart1985}.

Als Hyperparameter bezeichnet man in NN Konfigurationsparameter durch die man das Verhalten des Netzes steuern kann. Diese Variablen werden manuell festgelegt und nicht wie die übrigen Netz-Parameter vom Programm gelernt \cite[S. 113]{Goodfellow2015}.
%TODO Aktivierungsfunktion grafiken, tanh, relu, 
Laut Nwankpa, Ijomah, Gachagan \& Marshall beeinflussen sowohl Hyperparameter als auch Aktivierungsfunktionen die Generalisierbarbeit, sowie die Effektivität des Lernprozesses \cite{Nwankpa2018}. Dementsprechend wichtig ist es eine Aktivierungsfunktion auszuwählen, die möglichst gut für den spezifischen Anwendungsfall geeignet ist. Mithilfe von AF wird entschieden, ob ein Neuron gefeuert wird oder nicht. Der Output eines NN berechnet sich durch y = $(w_{1}x_{1}, w_{2}x_{2}, ..., w_{n}x_{n} + b)$, was grundsätzlich einem linearen Ergebnis entspricht. Für den Lernprozess werden jedoch nicht-lineare Daten benötigt, da diese differenzierbar sind. Daher bilden AF die Ausgabedaten eines NN innerhalb eines bestimmten Wertebereichs ab und wandeln sie in nicht-lineare Werte um. Daher ergibt sich, y = $ \alpha (w_{1}x_{1}, w_{2}x_{2}, ..., w_{n}x_{n} + b)$ für den Output, wobei $\alpha $ für die entsprechende AF steht \cite{Nwankpa2018}.

Ein Beispiel für eine Aktivierungsfunktion ist der tangens hyperbolicus (tanh), der durch 

$ f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ 

repräsentiert wird \cite{Nwankpa2018} und dessen grafischer Verlauf in Abbildung \ref{fig:tanh} ersichtlich wird. Die Funktionen bildet Eingaben auf einen Wertebereich zwischen -1 und 1 ab. Ihre Stärke ist, dass sie null-zentriert ist, ihre Schwäche hingegen, dass mit ihr weiterhin verschwindende Gradienten vorkommen können \cite{Nwankpa2018}.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{pics/tanh.png}	
\caption{Grafischer Verlauf der tanh-Funktion, Abbildung entnommen aus \cite[S. 45]{Kruse2015}}
\label{fig:tanh}
\end{figure}

Die rectified linear unit (ReLU) bietet ebenfalls die Möglichkeit zur Aktivierung der Neuronen und ist gegeben durch:


\begin{equation}
 f(x) = max (0, x) = 
\begin{cases}
      x_i, & \text{if $x_i \geq 0$}\\
      0, & \text{if $x_i<0$}
    \end{cases}
\end{equation}

Negative Eingabewerte werden hierbei auf den Null abgebildet, wodurch vanishing gradients vermieden werden können. Allerdings ist der gravierendste Nachteil dieser Funktion, dass sie zu sogenannten dead neurons führen kann \cite{Nwankpa2018}. 
Hinsichtlich dieses Problems kann jedoch die leaky ReLU (LReLU) Abhilfe schaffen, die die durch folgende Formel beschrieben wird:

\begin{equation}
 f(x) = \alpha x + x = 
\begin{cases}
      x, & \text{if $x > 0$}\\
      \alpha x, & \text{if $x \leq $}
    \end{cases}
\end{equation}

Durch die Multiplikation der Konstante $\alpha$, die den Wert 0.01 annimmt, können die Gradienten während des Trainings niemals Null werden \cite{Nwankpa2018}.

Für eine detaillierte Analyse bestehender Aktivierungsfunktionen sei auf Nwankpa et al. \cite{Nwankpa2018} verwiesen.

Ein weiteres Problem, bei dem Aktivierungsfunktionen Hilfestellung leisten ist das des vanishing beziehungsweise exploding gradient. Durch die wiederholte Multiplikation der Ableitungsterme, die entweder kleiner oder größer Eins sind, werden die Ergebnisse im Laufe der Zeit teils entweder verschwindend gering oder unendlich groß. AF können diese Werte durch entsprechende Abbildung innerhalb eines bestimmten Rahmens halten und dem Problem somit entgegenwirken \cite{Nwankpa2018}.

Ein weiteres wichtiges Hilfsmittel findet sich in der Batch Normalisation, mithilfe der Mittelwerte und Varianzen in den Eingabedaten der einzelnen Schichten normalisiert werden können. Das ist sinnvoll, da sich die Schichten sonst laufend an die neue Verteilung anpassen müssen. Der ausschlaggebende Vorteil der Normalisierung ist, dass das Modell deutlich effizienter trainiert werden kann, da höhere Lernraten angewendet werden können  \cite{Ioffe2015}.

Tiefe NN sind schwer zu trainieren. Zur Vereinfachung dieser Problematik besteht die Möglichkeit Abkürzungen zu nutzen. Die Vorgehensweise wird in Abbildung \ref{fig:res_skip} deutlich. Dabei können Daten (siehe \glqq{}x\grqq) bestimmte Schichten (vergleiche \glqq{}weight layer\grqq) überspringen und werden somit in diesen layern nicht verarbeitet, sondern gehen direkt weiter zu einem vorgegebenen Punkt im NN. Diese Methode wird \glqq{}residual learning\grqq{} genannt und optimiert den Umgang mit tiefen Netzen \cite{He2016}.

\begin{figure}
\centering
\includegraphics[width=0.4\textwidth]{pics/res_skip_connection.png}	
\caption{Darstellung einer möglichen Abkürzung in residual nets, Abbildung entnommen aus \cite{He2016}}
\label{fig:res_skip}
\end{figure}

Für detaillierte Erläuterungen zu NN wird auf das Grundlagenwerk \cite{Goodfellow2015} verwiesen.

\subsubsection{Neuronales Netz in Alpha(Go)Zero}
Der Input für das NN in AlphaZero besteht aus mehreren Ebenen, die die Größe des Spielfeldes annehmen. Für jeden Spielzustand existiert eine Kombination aus mehreren Ebenen, die diesen exakt abbilden. Dabei gibt es für jeden Input-Typ, beispielsweise Spielsteine von Spieler 1 oder von Spieler 2, eine solche Fläche, die durch binäre Codierung angibt, ob die entsprechende Eingabe auf der jeweiligen Position vorhanden ist oder nicht \cite{SilverHubert2017}.

Das in AlphaGo Zero und AlphaZero verwendete NN zeigt nachfolgenden von \cite{Silver2017} beschriebenen Aufbau.
Zunächst werden die Eingabedaten in einem \textit{residual block} verarbeitet. Dieser besteht aus einem \textit{convolutional block} und nachfolgend 19 oder 39 residualen Blöcken. Erst genannter führt eine Faltungsoperation mit 256 Filtern mit einem 3x3 Faltungskern und einer Schrittweite von 1 durch. Anschließend folgt die \textit{Batch Normalisation}, sowie die Aktivierung mithilfe der ReLU. Die Restblöcke verfügen ebenfalls über die drei genannten Komponenten – Faltung, Normalisierung und Aktivierung, sowie zusätzlich darauf anschließend erneut eine Faltung mit den gleichen Eigenschaften, gefolgt von der \textit{Batch Normalisation} und der Endposition der Abkürzungsmöglichkeit der Eingabedaten. Abschließend findet sich erneut die ReLU.
Die resultierenden Daten werden an den den sogenannten \glqq{}Policy Head\grqq{}, sowie den \glqq{}Value Head\grqq{} weitergereicht. Diese beiden Komponenten sind für das Berechnen der \textit{Policy} beziehungsweise der Spielbewertung zuständig. Der \textit{Policy Head} setzt sich aus einer Faltung mit zwei Filtern mit einem 1x1 Faltungskern und einer Schrittweite von 1, einer Normalisierung, der ReLU und einem \textit{fully connected layer} zusammen. Letzterer gibt einen Vektor zurück, der die Logit-Wahrscheinlichkeiten für alle realisierbaren Aktionen beinhaltet.
Die ersten drei Schritte sind gleichermaßen im \textit{Value Head} wiederzufinden. Allerdings folgt hier ein \textit{fully connected layer} mit einer versteckten Schicht. Des Weiteren durchlaufen die Daten die ReLU und erneut eine vollständig verknüpfte Schicht, die in einem Skalar resultiert. Dieser wird abschließend durch die Aktivierungsfunktion tanh auf einen Punkt im Wertebereich zwischen [-1, 1] abgebildet \cite{Silver2017}.

Letztendlich beschränkt sich die der Output des NN auf zwei Komponenten, nämlich dem policy und dem value head. Der policy head gibt für jede Position im Spielfeld eine A-priori-Wahrscheinlichkeit zurück, die angibt mit wie wahrscheinlich es ist, dass der Spieler seinen Spielstein auf dieses Feld legt. Der value head hingegen bewertet den Spielzustand und gibt an, wie wahrscheinlich es ist, dass der Spieler in diesem Zustand gewinnt \cite{Silver2017}.

Trainiert wird das NN durch Spiele gegen sich selbst. In AlphaGo Zero gibt es dabei zwei Instanzen. Das neu trainierte Modell wird nach jedem Trainingsdurchlauf mit dem derzeit besten Modell verglichen. Falls das neue Modell in 55 \% der Spiele gegen das beste gewinnt, so wird das beste Modell überschrieben. In beiden Varianten werden die Parameter zunächst zufällig initialisiert. Für AlphaZero gibt es aber lediglich eine Instanz, deren Parameter laufend aktualisiert werden \cite{SilverHubert2017}. 

\subsection{Reinforcement Learning}
Reinforcement Learning (RL), auch als bestärkendes Lernen bezeichnet, beschreibt die Interaktion eines Agenten mit einer konkreten Environment. Dabei stellt die Umgebung dem Agenten einen bestimmten Zustand bereit, anhand dessen er die bestmögliche Aktion auswählt, die im nächsten Schritt ausgeführt werden soll. Daraufhin wird der Zustand entsprechend modifiziert und eine Funktion berechnet, die dem Agenten eine Belohnung oder Bestrafung in Form eines positiven beziehungsweise negativen Rewards zurückliefert. Anhand iterativer Durchläufe von Lern- und Entscheidungsprozessen optimiert der Agent seine Aktionen. Dies ist in Abbildung \ref{fig:rl_agent_loop} einzusehen. Dabei wählt der Agent zunächst Aktion $a_{t}$ im Zustand $s_{t}$, was eine Änderung des Zustands der Umgebung zu $s_{t+1}$ zur Folge hat. Außerdem berechnet die Environment den Reward $r_{t}$ für die vom Agenten gewählte Aktion. Ziel in diesem Trainingsdurchlauf für den Agenten ist es, anhand der Belohnungsstrategie die Entscheidung bezüglich der besten nächsten Aktion zu stetig zu verbessern. Hierbei anzumerken ist, dass die Rewardfunktion sowohl den Zustand als auch die Aktion berücksichtigt, um einen Rückgabewert zu berechnen. Daraus folgt, dass die gleiche Aktion unterschiedlichen Ausgangszuständen in verschiedenen Belohnungswerten resultieren kann beziehungsweise sogar sollte \cite[S. 2]{Sewak2019}.

Die Strategie zur Ermittlung der besten Aktion während des Lernprozesses wird als Policy $\pi$ bezeichnet. Diese ist während eines gesamten Trainingsdurchlaufs gültig. Bezüglich der Notation gibt $\pi_{(s)}$ entsprechend die vielversprechendste Aktion im Zustand s an \cite[S. 16]{Sewak2019}.

\begin{figure}
\centering
\includegraphics{pics/rl_agent_loop.png}	\caption{iterative Interaktion zwischen Agent und Environment, Abbildung entnommen aus \cite[S. 2]{Sewak2019}}
\label{fig:rl_agent_loop}
\end{figure}

Ferner bringt das Aufstellen einer Rewardfunktion Schwierigkeiten mit sich, da sie versucht eine sinnvolle Vorhersage für die Bewertungen eines Zustandes zu ermitteln. Probleme können dabei beispielsweise darin liegen, dass Belohnungen womöglich erst im zukünftigen Verlauf zu Trage treten können oder dass sie zum gegenwärtigen Zeitpunkt schlichtweg noch ungewiss sind. Mögliche Lösungen hierfür sehen allerdings je nach Anwendungsdomäne verschieden aus \cite[S. 4-7]{Sewak2019}.

Reinforcement Learning Modelle sind als \glqq Markov Devision Process\grqq{} (MDP) anzusehen. Das hat zur Folge, dass aufgrund der zugrundeliegenden \glqq Markov Property, sowie Markov Chain\grqq{} die Wahrscheinlichkeiten für die nächstmöglichen Aktionen lediglich vom derzeitigen Zustand und nicht von weiteren vorangegangen abhhängen. Der MDP wendet diese Regeln auf den Entscheidungsprozess an, der im Fall von RL die Policy darstellt. Des Weiteren liefert der MDP die Wahrscheinlichkeiten für Zustandsübergange in der Form $P_{a}(s, s')$ an, wobei a für die mögliche Aktion, s für den aktuellen und s' für den resultierenden Zustand steht. Die Notation des entsprechenden Rewards lautet $R_{a}(s, s')$ \cite[S. 19f.]{Sewak2019}.

Weitere Begriffe, die im Kontext von RL von Bedeutung sind, sind \glqq{}Policy Evaluation\grqq{} und \glqq{}Policy Iteration\grqq{}. Erstere steht für die geschätzte Bewertung des Zustandes. Letztere beschreibt einen Iteration Prozess mit dem Ziel, dass die Policy konvergiert \cite[S. 27]{Sewak2019}.

\subsection{Zusammenspiel von Monte Carlo Tree Search und neuronalem Netz}
AlphaGoZero und AlphaZero basieren auf der Kombination des MCTS und des NN. Dabei gibt es zwei Schnittstellen zwischen diesen beiden Komponenten. 
Erstere liegt in der bereits erwähnten Bewertung von Blattknoten. Der MCTS durchläuft keinen simulierten Spielablauf, sondern überlässt die Evaluation des Knotens dem NN und arbeitet mit den zurückgegeben Daten weiter. Dies spiegelt die policy evaluation wider \cite{Silver2017}.

Eine weitere Verzweigung von MCTS und NN tritt bei dem Update der Parameter des NN auf. Das NN ermittelt zu jedem Spielstand die möglichen Züge und gibt deren Gewinnwahrscheinlichkeit, sowie die Wahrscheinlichkeit für die Auswahl des Zuges an. Für die Aktualisierung der Parameter werden die genannten Werte dahingehend angepasst, dass sie den im MCTS ermittelten Werten entsprechend beziehungsweise sich diesen annähern. Eine Anpassung in diese Richtung ist sinnvoll, da die Daten im MCTS als deutlich genauer gelten. Dieser Vorgang entspricht der policy improvement \cite{Silver2017}.

\newpage
% ----------------------------------------------------------------------------------
% Kapitel: Implementierung/Umsetzung
% ----------------------------------------------------------------------------------
\section{Implementierung}

\subsection{Aufbau des Programms}
%TODO klassen, kommunikation etc.

\subsection{Monte Carlo Tree Search}
Um den MCTS für Reversi zu realisieren wurden die Klassen MCTS und Node angelegt. Letztere enhält zwei überladene Konstruktoren zum Anlegen von Wurzel- und Kindknoten. Ein Node enthält zusätzliche Attribute. Sowohl der Elternknoten als auch eine ArrayList vom Typ Node, die die direkten Kinder enthält, werden abgespeichert. Die Anzahl, wie oft ein Knoten besucht wurde, wird in der Variable \texttt{numVisited} hinterlegt. Außerdem gespeichert wird das Ergebnis eines simulierten Spieldurchlaufs in \texttt{simulationReward}. Der aktuelle Spielzustand, der im Playground repräsentiert wird, wird ebenfalls im Node hinterlegt. Des Weiteren wird in dem Attribut \texttt{nextPlayer} festgehalten, welcher Spieler als Nächstes an der Reihe ist und welche Züge (\texttt{nextTurns}) dieser als nächstes ausspielen kann. Für das Training werden außerdem die vom NN erhaltenen A-Priori-Wahrscheinlichkeiten abgespeichert. Dies geschieht allerdings ausschließlich, wenn der dem \texttt{Node} zugehörige \texttt{Turn} vom Agent gespielt wird.

Es gibt einen überladenen Konstruktor, der einerseits für das Anlegen einer neuen Wurzel und andererseits für das Erzeugen eines neuen Kinderknotens zuständig ist. Bei der Instanziierung durch die Konstruktoren werden sinnvolle initiale Werte vergeben. \texttt{numVisited} und \texttt{simulationReward} werden auf 0 beziehungsweise 0.0  für den \texttt{root}, und 1 beziehungsweise dem vom NN erhaltenen, übergebenen \texttt{reward} gesetzt. Für den Wurzelknoten gilt, dass er keinen Parent besitzt, für alle weiteren Knoten wird der Parent übergeben und gesetzt. Die Children werden zunächst durch eine leere Liste initialisiert. Der \texttt{nextPlayer} wird ebenfalls übergeben und gesetzt. Außerdem zu erwähnen ist die Methode \texttt{calculateUCT()}, die den Upper Confidence Bound applied to Trees (UCT) für einen Knoten berechnet. Sie ermittelt zunächst die Exploitation-Komponente, indem sie den simulationReward durch die Anzahl an Besuchen dividiert. Die Exploration berechnet sich aus dem Verhältnis, wie oft der Parent besucht wurde, geteilt durch den inkrementierten Wert, wie oft der aktuelle Knoten besucht wurde. Aus dem Quotient wird anschließend die Wurzel gezogen. Um den Kompromiss zwischen diesen beiden Komponenten zu kontrollieren, wird die Exploration mit einer festgelegten Konstante und der A-priori-Wahrscheinlichkeit für einen Zug multipliziert. Diese wird im neuronalen Netz trainiert.
%TODO Konstante Wert

Da die Implementierung des MCTS vor der des NN stattfand, entstanden zwei lauffähige Versionen, sodass einerseits eine Baumsuche ohne Zusammenarbeit mit dem NN , andererseits aber mithilfe des NN durchgeführt werden kann. Da in erst genannte ebenfalls viel Aufwand investiert wurde, bleibt diese zusätzlich erhalten. Dafür spezifische Methoden wurden mit \glqq{}\texttt{\_deprecated}\grqq{} versehen.

Hinsichtlich der Klasse \texttt{MCTS} ist festzuhalten, dass diese das Interface \texttt{ITurnChoiceAlgorithm} implementiert und in der Klasse \texttt{Agent} über den Konstruktoraufruf instanziiert wird. Dieser verlangt das Environment und den Player als Übergabeparameter und legt daraufhin einen neuen Wurzelknoten an, sowie eine leere \texttt{ArrayList} vom Typ \texttt{Node}, die die Blattknoten beinhaltet, die im späteren Verlauf simuliert werden müssen. Für die Simulation muss beachtet werden, dass die Environment geklont und somit eine tiefe Kopie erzeugt werden muss, damit der tatsächliche Spielzustand nicht unbeabsichtigt manipuliert wird. 

Der nächste Abschnitt widmet sich dem Code zur Baumsuche im Zusammenspiel mit dem NN, dessen Ablauf in Abbildung\footnote{Erstellt mit astah (https://astah.net/)} \ref{fig:mcts_seq} veranschaulicht wird. Zunächst wird über die öffentliche Interface-Methode \texttt{chooseTurnPhase1()} die private Methode \texttt{searchBestTurn()} aufgerufen. Hierbei wird zunächst über die globale Variable \texttt{firstCall} überprüft, ob die Baumsuche mit einem neuen Wurzelknoten startet oder ob der nächste zu evaluierende Knoten ausgewählt werden muss. Für letzteren Fall gilt, dass im \texttt{else}-Block der Knoten mit dem größten UCT-Wert ausgehend von der Wurzel ermittelt wird. Dazu wird \texttt{searchBestUCT()} aufgerufen. Falls man sich im ersten Aufruf befinden, wird \texttt{root} zur Evaluation an das NN übergeben. Dies geschieht durch den Aufruf von \texttt{evaluate(playground, player)} aus der Klasse \texttt{PolicyValuePredictor}, wobei der entsprechende Spielzustand und Spieler übergeben werden. Als Rückgabe erhält man die Zustandsbewertung (\texttt{reward}) und die A-prior-Wahrscheinlichkeiten \texttt{priors} als Array vom Typ \texttt{double}. Diese \texttt{priors} signalisieren die Wahrscheinlichkeit, dass ein der Spieler im nächsten Zug einen Spielstein auf ein bestimmtes Feld setzt. Da dabei alle Positionen im Spielfeld berücksichtigt werden, werden anschließend lediglich die gültigen Züge mithilfe der Funktion \texttt{getPossibleTurns} herausgefiltert und um ihre jeweilige A-priori-Wahrscheinlichkeit ergänzt. Da die Wurzel nun bewertet wurde, werden die nächstmöglichen Züge (\texttt{setNextTurns(validTurns)}), sowie ihre Zustandsbewertung (\texttt{setSimulationReward(reward)}) gesetzt. Außerdem wird die hinterlegte Anzahl der Besuche im \texttt{root} inkrementiert (\texttt{incNumVisited}). Abschließend wird der Wurzelknoten als \texttt{bestUCTNode} hinterlegt und \texttt{firstCall} wird auf \texttt{FALSE} gesetzt. 

Unabhängig davon, welche Ausgangssituation vorlag, wird als nächstes eine \texttt{while}-Schleife durchlaufen, die erst abbricht, wenn es keinen weiteren Folgeknoten zur Evaluation gibt (\texttt{bestUCT == null}) oder sobald 1600 Simulationen (siehe Konstante \texttt{NR\_SIMULATIONS} ) durchgeführt wurden. Innerhalb der Schleife wird stets ein Knoten mithilfe der Funktion \texttt{createNextNodeAndEvaluate(bestUCTNode)} evaluiert. Diese sucht zunächst durch die Funktion \texttt{choseNextTurn} den Zug aus, der ausgehend von dem übergebenen Knoten als nächstes gemacht werden soll. Das Entscheidungskriterium hierfür liegt in der höchsten A-priori-Wahrscheinlichkeit. Anschließend wird der \texttt{playground} entsprechend dem ausgewählten Zug aktualisiert und zur Bewertung an das NN übergeben. An dieser Stelle passiert selbiges wie oben bereits für den Wurzelknoten beschrieben. Abschließend wird jedoch ein neuer Knoten für den durchgeführten \texttt{turn} erstellt, der anfangs übergebene Knoten erhält den neuen Knoten als Kind und der betrachtete Spielzug wird aus der Liste der nächstmöglichen Züge entfernt.

Nachdem die Evaluation abgeschlossen ist, wird der ermittelte Wert für den Knoten anhand der Methode \texttt{backpropagate()} zurück übermittelt. Dabei wird der \texttt{reward} des übergebenen \texttt{node} an alle in der Hierarchie über ihm stehenden Knoten übermittelt. Zusätzlich wird die Anzahl an Besuchen der entsprechenden Knoten inkrementiert. Daraufhin wird in der Methode \texttt{setBestTurn} der Zug ermittelt, der aktuell als am besten interpretiert wird. Ausschlaggebend hierfür ist, welcher Knoten am öftesten besucht wurde. Abschließend wird erneut der als nächstes zu evaluierende Knoten durch \texttt{searchBestUCT} ermittelt, wodurch die Iteration erneut beginnt.

\begin{figure}
\centering
\includegraphics[width=1.0\textwidth]{pics/mcts_seq.png}	
\caption{Sequenzdiagramm der wichtigsten Komponenten des MCTS, eigene Abbildung erstellt mit astah}
\label{fig:mcts_seq}
\end{figure}

Nachfolgend wird die Implementierung für den MCTS ohne NN erläutert. Beim Klonen der Environment ist festzuhalten, dass dies zweimal stattfindet. Einmal, wenn ein neuer Baum aufgebaut wird, somit erhält der neue Wurzelknoten und ebenfalls jedes seiner Kinder jeweils einen eigenen Klon. Für die Kinderknoten gilt, dass diese ihre Environment-Instanz an ihre Kinder weitergeben und diese somit innerhalb derselben Instanz agieren. Um den MCTS zu starten, wird die Methode \texttt{searchBestTurn()} aufgerufen. Diese expandiert zunächst den Wurzelknoten, indem sie alle im aktuellen Zustand möglichen validen Züge ermittelt und durch diese iteriert. Die Methode \texttt{getPossibleTurns()} gibt diese zurück. Sie iteriert über das gesamte Spielfeld und prüft dabei mithilfe der Methode \texttt{validateTurnPhase1()} im Environment, an welcher Stelle ein gültiger Zug gemacht werden kann. Für jeden dieser Züge wird in \texttt{expand()} der Kinderknoten angelegt, sowie als unbesuchter Blattknoten abspeichert. Außerdem wird ermittelt, welcher Spieler als Nächster einen Zug machen darf und der simulierte derzeitige Spielzustand anhand des Zuges aktualisiert. Nachdem ein Knoten expandiert wurde, wird er aus der Liste der Blattknoten wieder entfernt.
Daraufhin werden die unbesuchten Blattknoten, die am Anfang den Kinderknoten der Wurzel entsprechen, in der Methode \texttt{traverse()} durchlaufen. Dabei wird in jedem dieser eine Simulation gestartet, die einen Spielverlauf bis zum Spielende anhand zufällig ausgewählter möglicher Züge durchspielt. 

Mithilfe der Funktion \texttt{simulate()} erfolgt die Simulation eines Spiels. Ein Zufallszug wird durch eine Instanz der Java-Klasse Random generiert. Hierbei wird ein zufälliger Integer erzeugt, der durch eine Modulo-Operation auf den Größenbereich abgestimmt wird, der dem der Anzahl der möglichen Züge entspricht. Die resultierende Zahl gibt den auszuwählenden Zug innerhalb der ArrayList an.

Wenn keine weiteren validen Folgezüge ermittelt werden können, bedeutet das das Spielende und der Reward für die Spielausgang wird anhand der Funktion \texttt{rewardGameState()} berechnet. Diese erhält als Parameter das Environment, sowie den Spieler, für den der Reward kalkuliert werden soll. Indem der gesamte Playground durchlaufen und gezählt wird, wie viele Steine vom übergebenen Spieler enthalten sind, errechnet sich die Bewertung des Spiels. Abschließend werden die Werte für Anzahl Besuche und Reward ebenfalls im Wurzelknoten aktualisiert.

Nach Abschluss der Simulation wird der Reward zurückgegeben. Daraufhin wird in \texttt{traverse()} die Backpropagation der Ergebnisse durchgeführt, indem iterativ vom aktuellen Knoten bis hoch zur Wurzel die Anzahl an Besuchen inkrementiert und der Reward entsprechend erhöht wird.

\subsection{Neuronales Netz}

\newpage

% ----------------------------------------------------------------------------------
% Kapitel: Training
% ----------------------------------------------------------------------------------

\section{Training des neuronalen Netzes}



\newpage

% ----------------------------------------------------------------------------------
% Kapitel: Allgemeine Informationen/Organisation
% ----------------------------------------------------------------------------------
\section{Organisation}

\subsection{Team und Aufgabenverteilung}
% Beschreiben Sie in diesem Abschnitt Ihr Team. Welche Person hat welche Aufgaben wahrgenommen, wie wurden
% Aufgaben aufgeteilt und wie wurde kommuniziert, etc.
Als Vierer-Team bestand unsere Gruppe aus Simon Hofmeister, Nadiia Matsko, Monika Silber und Simon Wasserburger. Anzumerken ist, dass Simona Hofmeister und Simon Wasserburger bereits im Bachelorkurs Erfahrung mit dem Implementieren einer künstlichen Intelligenz für Reversi sammeln konnten. Aus diesen Gründen griffen wir für den Aufbau dieses Projektes auf das Grundgerüst des bereits im Bachelorkurs erstellen Programms von Simon Hofmeister zurück, der dieses entsprechend an die Anforderungen des vorliegenden Projektes anpasste.

Hinsichtlich der Verteilung der Aufgaben und des zugehörigen Aufwands ergab sich für die einzelnen Teammitglieder folgendes:
%TODO Aufdröselung in Recherche, Implementierung, Doku und Orga
Die weiteren Komponenten verteilten wir wie folgt:

Simon Hofmeister (150 h):
\begin{itemize}
\item Recherche
\item Implementierung (Grundgerüst, NN)
\item Dokumentation ()
\item Organisation
\end{itemize}       

Nadiia Matsko (150 h): 
\begin{itemize}
\item Recherche
\item Implementierung (NN)
\item Dokumentation (AlphaGo, AlphaZero)
\item Organisation
\end{itemize}  

Monika Silber (150 h): 
\begin{itemize}
\item Recherche
\item Implementierung (MCTS)
\item Dokumentation (Reversi, Theoretische Grundlagen, Implementierung MCTS, Organisation)
\item Organisation
\end{itemize}  

Simon Wasserburger (150 h): 
\begin{itemize}
\item Recherche
\item Implementierung (MCTS, Code-Refactoring Grundgerüst)
\item Dokumentation (Grafiken MCTS)
\item Organisation
\end{itemize}  
%TODO Ergänzungen

\subsection{Kommunikation}
Die virtuelle Kommunikation fand hauptsächlich in unserer WhatsApp-Gruppe statt, wo wir einander auf Problemstellen aufmerksam machten, Lösungsansätze diskutierten, einander über Fortschritte informierten und Treffen ausmachten. Ebenfalls konnte durch Commits im Repository die Arbeit der Teamkollegen und entsprechend der Projektfortschritt verfolgt werden. Daneben verfügten wir über ein Trello-Board, in dem wir Aufgaben planten, dokumentierten und zuwiesen. In gemeinsamen Treffen besprachen und ergänzten wir dabei die offenen Punkte und kontrollierten den Projektablauf. 

In den persönlichen Treffen, in denen meist alle Teammitglieder anwesend waren, konnten wir alle offenen Punkte konkretisieren und ausführlich diskutieren. Wir erarbeiteten gemeinsame Lösungsstrategien und halfen einander bei Verständnisproblemen. Abschließend trafen wir Absprachen zum weiteren Vorgehen, sowie zur Aufgabenverteilung. In Ausnahmefällen erfolgte dies online per Sprachkonferenzen.

\subsection{Versionskontrolle}
Als Versionskontrollsystem stand ein Gitlab-Repository der OTHR zur Verfügung. Zum Abrufen und Bearbeiten dessen wurde GitHub Desktop als GUI verwendet.
%TODO welche GUI/CLI?

Hinsichtlich Implementierung der Komponenten wurden eigene Branches angelegt, die vom master-Branch geklont und nach Fertigstellung entsprechend dort wieder gemergt wurden. Dabei ergaben sich eigene Branches für den MCTS, das NN, sowie für den Prozess des Refactoring des ursprünglichen Codes. Die finale und alle Bestandteile umfassende Version befindet sich folglich im master-Branch.

\subsection{OS, IDE und Programmiersprache}
%TODO OS
Als Entwicklungsumgebung wurde IntelliJ IDEA\footnote{https://www.jetbrains.com/de-de/idea/} von JetBrains genutzt. Da wir innerhalb der Gruppe bisher am meisten Erfahrung in Java hatten, wurde dies als Programmiersprache zur Implementierung des Projektes verwendet. Zur Erstellung des Programms kam maven\footnote{https://maven.apache.org/} als Build-Management-Tool zum Einsatz.

Des Weiteren wurde zur Implementierung des NN die Library Eclipse deeplearning4j (DL4J) \cite{DL4J} zur Hilfe genommen. Hinsichtlich dieser Auswahl verschafften wir uns zunächst einen Überblick über vorhandene, gute Bibliotheken und recherchierten deren Vorteile und Anwendungsfälle, wobei die Entscheidung letztendlich auf die bereits genannte fiel.

Da wir uns innerhalb der Gruppe auf die Programmiersprache Java geeignet hatten, suchten wir dementsprechend eine deep-learning-Library für Java. Zwar stellt beispielsweise das bekannte Python-Framework TensorFlow\footnote{https://www.tensorflow.org/} eine Java API bereit, diese besitzt jedoch experimentellen Charakter\footnote{$https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/package-summary$} und ist daher für unsere Zwecke ungeeignet. 

Ebenfalls die beliebte Python-Bibliothek Keras\footnote{https://keras.io/} bietet Möglichkeiten zur Anbindung an Java. Zwar können in Keras trainierte Modelle mithilfe von DL4J nach Java importiert werden, jedoch müssen diese zuvor in Python konfiguriert und trainiert werden\footnote{https://deeplearning4j.org/docs/latest/keras-import-overview}.

Da DL4 hinsichtlich unserer individuellen Projektansprüche am besten schien, fiel die Entscheidung letztlich auf diese Wahl. Entsprechend folgt nun die Darlegung der Eigenschaften von DL4J, wobei der Fokus auf der Ausarbeitung der für das vorliegende Projekte relevanten Aspekten liegt. Sämtliche Informationen sind, sofern nicht anders angegeben, der zugehörigen Website (https://deeplearning4j.org/) entnommen.

Die Library hat ihren Ursprung bei Entwicklern der Firma Konduit\footnote{https://konduit.ai/} in San Francisco und wurde unter der Apache Software Foundation License 2.0\footnote{https://www.apache.org/licenses/LICENSE-2.0} veröffentlicht.

DL4J ist vollständig open source und in Java geschrieben, wobei für die zugrundeliegenden Berechnungen C, C++ und Cuda genutzt wurden. Des Weiteren unterstützt sie ND4J (N-Dimensional Arrays for Java) - eine Bibliothek für das wissenschaftliche Rechnen, die eine hohe Performanz gewährleistet\footnote{https://nd4j.org/}. Diese werden im vorliegenden Projekt anhand von \texttt{INDArrays} referenziert.
%TODO INDArrays

Außerdem werden zahlreiche, hilfreiche Features unterstützt, unter anderem Batch Normalisation, Residual Learning, CNNs. In Form eines \texttt{ComputationGraph} kann daher ein verhältnismäßig einfach ein komplexes neuronales Netz aufgebaut werden.

Weitere ausschlaggebende Vorteile liegen in der ausführlichen Dokumentation\footnote{https://deeplearning4j.org/api/latest/}, sowie den zahlreichen Hilfsmitteln, wie Tutorials\footnote{erreichbar über https://deeplearning4j.org/tutorials/setup}, Code-Beispiele\footnote{https://github.com/eclipse/deeplearning4j-examples} und Leitfäden zu sämtlichen Aspekten\footnote{erreichbar über https://deeplearning4j.org/docs/latest/}, die zudem alle schnell und unkompliziert über die eigene Website zu finden sind.

Zuletzt können wir nach Projektabschluss festhalten, dass wir DL4J als sehr benutzerfreundlich und unkompliziert empfanden. Ebenfalls der große Funktionsumfang erleichtert die Arbeit zum Aufbau und Training des Modells. Daher sehen wir DL4J insgesamt als hilfreiches und anwenderfreundliches Tool.

%TODO Alternativen, abschließenden Bewertung (Kritikpunkte?)

\subsection{Testumgebungen}

\subsection{Projekt-Dokumentation}

\newpage
% ----------------------------------------------------------------------------------
% Kapitel: Fazit
% ----------------------------------------------------------------------------------
\section{Fazit}

Subjektiv betrachtet ist die Einarbeitung in AlphaGo Zero und AlphaZero sehr mühsam und langwierig. Die originalen Publikationen setzen einerseits teils viel Grundwissen voraus, andererseits lassen sie häufig Interpretationsfreiraum bei gegebenen Erläuterungen. Diese beiden Faktoren machen weitere Recherchen und das Aufsuchen anderweitiger Quellen unabdingbar. Zwar finden sich oftmals Blog-Einträge und dergleichen im Internet, jedoch werden dort meist unterschiedliche konkrete Umsetzungsvarianten verwendet, was einheitliche Auffassung erheblich erschwert. Nicht zu vernachlässigen ist hierbei, dass innerhalb der eigenen Recherche der Teammitglieder somit jeder etwas anderes liest und eine andere Auffassung hat. An dieser Stelle auf einen gleichen Nenner zu kommen und zu belegen, wie die tatsächlich richtige Umsetzung aussieht, gestaltet sich schwierig, besonders dann, wenn im Paper nichts konkretes dazu zu finden ist. Des Weiteren möchte man sich auf diese Internet-Quellen nicht unbedingt verlassen und ebenso lehnt man die Referenzierung in der wissenschaftlichen Ausarbeitung ab. Außerdem ähneln beziehungsweise unterscheiden sich die AlphaGo Zero und AlphaZero in manchen Punkten. Da die Publikation für AlphaZero alleine nicht vollständig ist, sondern das Nachlesen im AlphaGo Zero Paper nötig ist, wird es einem nochmals erschwert die beiden Ansätze klar zu trennen. Alle diese Aspekte führen dazu, dass die genaue Analyse des Ansatzes und der Vorgehensweise umständlich ist und einen enormen Zeitaufwand erfordert. 

Abgesehen davon ist festzuhalten, dass die beiden Komponenten, die den Alpha(Go)-Zero-Ansatz ausmachen, das NN und der MCTS, beide für sich betrachtet bereits umfangreich und komplex sind. Bezüglich des Projektberichts, der die theoretische Auseinandersetzung und verständliche Erklärung der Komponenten erfordert, ist anzumerken, dass dies vor allem im Hinblick auf das NN schlichtweg unmöglich ist. Generelle Funktionsweise und zusätzliche Elemente, unter anderem Verlust- und Aktivierungsfunktionen, Gradientenabstiegsverfahren, sowie weitere Abwandlungen und Besonderheiten, beispielsweise residual learning oder Momentum, füllen vom Umfang eine gesamte Vorlesung über ein Semester. Beim Versuch all diese Konzepte, die nun mal im Projekt Anwendung finden und daher eine Erklärung erfordern, müssen deutliche Abstriche gemacht werden, da der Aufwand für eine detaillierte Darlegung dieser die zu erbringende Projektleistung deutlich übersteigt.

Rückblickend betrachtet hätten wir die Herangehensweise an das Projekt anders gestalten sollen. Da die Analysephase viel Zeit in Anspruch nahm, wir dennoch sichtliche Fortschritte erzielen wollten, begannen wir mit der praktischen Umsetzung, bevor wir alle theoretischen Aspekte detailliert ausgearbeitet hatten. Dies führte unweigerlich dazu, dass bereits bestehende Funktionalitäten erneut überarbeitet und angepasst werden mussten, was einem Zusatzaufwand entsprach. Für die Zukunft lernen wir daraus, dass es sinnvoll ist, erst mit der Umsetzung anzufangen, sobald die gesamte Theorie behandelt und konkretisiert worden ist.

Ein weiterer Punkt, in dem wir erst dazu lernen mussten, betraf die Kommunikation. Sie funktionierte zwar innerhalb der Gruppe gut, jedoch musste sie zum Dozenten verbessert werden. In diesem Punkt waren vermehrt Angaben zum Fortschritt, Adressierung von Problemstellen und Details zum geplanten Vorgehen gewünscht. Eine solche Arbeitsweise waren wir aufgrund unserer bisherigen Projekte, die auf den Dialog innerhalb des Team begrenzt war, nicht gewohnt. Aus diesem Grund hatten vernachlässigten wir diesen Aspekt offensichtlich zu sehr, aber bemühten uns jedoch im Laufe der Arbeit durch vermehrten E-Mail-Kontakt diesem besser nachzukommen, indem wir nach Treffen sämtliche Informationen zusammentrugen und kommunizierten.

Ein weiterer als negativ empfundener Aspekt lag in der benötigten Zeit für das Training des Modells des NN. Dadurch, dass das NN stets längere Zeit brauchte, um zu lernen und sich zu verbessern, hat sich die gesamte Projektdauer verlängert.

%TODO Zeitaufwand konkret
%TODO Abschließendes Fazit

% Beschreiben Sie in diesem Abschnitt u.a.\ was Ihnen an diesem Fach gefallen hat und welche
% Verbesserungsvorschläge Sie für künftige Veranstaltungen haben. Was konnten Sie dazulernen, in welchen
% Bereichen haben Sie sich verbessert. Welche Problemsituationen gab es während der Projekterstellung, wie
% sind Sie diese angegangen und wie haben Sie diese gelöst. Was haben Sie evtl.\ vermisst.

\pagebreak

%\newpage
% ----------------------------------------------------------------------------------
% Kleine Einführung in LaTeX-Elemente
% ----------------------------------------------------------------------------------
% \section{\LaTeX-Elemente}
% Dieser Abschnitt soll nicht Bestandteil des Projektberichtes sein, sondern beinhaltet lediglich einige
% Informationen über \LaTeX-Distributionen, Editoren und \LaTeX-Elemente, die Ihnen beim Einstieg in das
% \LaTeX-Textsatzsystem helfen sollen.

% \subsection{\LaTeX-Distributionen nach Betriebssystemen}

% \subsubsection{\LaTeX-Distributionen}

% Folgende Haupt-\LaTeX-Distributionen stehen Ihnen zur Verfügung:
% \begin{itemize}
%  \item Windows:\quad \texttt{MiKTeX}\quad Webseite:\quad\url{http://www.miktex.org}
%  \item Linux/Unix:\quad \texttt{TeX Live}\quad Webseite:\quad\url{http://tug.org/texlive/}
%  \item Mac OS:\quad \texttt{MacTeX}\quad Webseite:\quad\url{http://www.tug.org/mactex/}
% \end{itemize}

% \subsubsection{\LaTeX-Editoren}
% Auf folgenden Webseiten können Sie einige hilfreiche \LaTeX-Editoren finden:
% \begin{itemize}
%  \item Windows/Linux/Mac OS: \url{http://www.xm1math.net/texmaker/}
%  \item Windiws: \url{http://www.texniccenter.org/}
%  \item Mac OS: \url{http://pages.uoregon.edu/koch/texshop/}
% \end{itemize}

%Falls bei den oben genannten Editoren kein passender vorhanden war, findet sich auf Wikipedia eine  %Zusammenstellung vieler weiterer \LaTeX-Editoren:\\[1em]
%\hspace*{3cm}\url{https://en.wikipedia.org/wiki/Comparison_of_TeX_editors}


%\subsection{Unterabschnitt}
%Zum Einfügen eines Bildes, siehe Abbildung \ref{fig:reversi01}, wird die
% \textit{minipage}-Umgebung 
%genutzt, da die Bilder so gut positioniert werden können.

%\vspace{1em}
%\begin{minipage}{\linewidth}
%	\centering
%	\includegraphics[width=0.6\linewidth]{pics/gamefield01.png}
%	\captionof{figure}[Spielfeld 01]{Unbespieltes Spielfeld\footnotemark }
%	\label{fig:reversi01}
%\end{minipage}
%\footnotetext{Diesem Spielfeld wurden noch keine Spieler zugewiesen (daher die
% dunklen Spielsteine)}

%Nachdem das Spiel gestartet wurde und beiden Spielphasen durchlaufen wurden, siegt
% schließlich der %Spieler mit der Farbe rot.

%\vspace{1em}
%\begin{minipage}{\linewidth}
%	\centering
%	\includegraphics[width=0.6\linewidth]{pics/gamefield02.png}
%	\captionof{figure}[Spielfeld 02]{Finales Spielfeld\footnotemark }
%	\label{fig:reversi2}
%\end{minipage}
%\footnotetext{Das Spielfeld nach der Zug- und Bombenphase. Spieler rot gewinnt
% eindeutig.}

%\subsection{Tabellen}
%In diesem Abschnitt wird eine Tabelle (siehe Tabelle \ref{tab:beispiel}) dargestellt.

%\vspace{1em}
%\begin{table}[!h]
%	\centering
%	\begin{tabular}{|l|l|l|}
%		\hline
%		\textbf{Name} & \textbf{Name} & \textbf{Name}\\
%		\hline
%		1 & 2 & 3\\
%		\hline
%		4 & 5 & 6\\
%		\hline
%		7 & 8 & 9\\
%		\hline
%	\end{tabular}
%	\caption{Beispieltabelle}
%	\label{tab:beispiel}
%\end{table}


%\subsection{Auflistung}
%Für Auflistungen wird die \textit{enumerate}- oder \textit{itemize}-Umgebung genutzt.
%
%\begin{itemize}
%	\item Nur
%	\item ein
%	\item Beispiel.
%\end{itemize}
%
%\subsection{Listings}
%Zuletzt ein Beispiel für ein Listing, in dem Quellcode eingebunden werden kann, siehe Listing \ref{lst:arduino}.
%
%\vspace{1em}
%\begin{lstlisting}[caption=Arduino Beispielprogramm, label=lst:arduino]
%int ledPin = 13;
%void setup() {
%    pinMode(ledPin, OUTPUT);
%}
%void loop() {
%    digitalWrite(ledPin, HIGH);
%    delay(500);
%    digitalWrite(ledPin, LOW);
%    delay(500);
%}
%\end{lstlisting}
%
%\subsection{Tipps}
%Die Quellen befinden sich in der Datei \textit{quellen.bib}. Eine Buch- und eine Online-Quelle sind beispielhaft eingefügt. [Vgl. \cite{buch}, \cite{online}]
%
%\pagebreak

% ----------------------------------------------------------------------------------------------------------
% Literatur
% ----------------------------------------------------------------------------------------------------------
\renewcommand\refname{Literatur}
\bibliographystyle{alpha}
\bibliography{literatur}

%\printbibliography

\pagebreak

% ----------------------------------------------------------------------------------------------------------
% Anhang
% ----------------------------------------------------------------------------------------------------------
\pagenumbering{Roman}
\setcounter{page}{1}
%\lhead{Anhang \thesection}

\begin{appendix}
\section*{Anhang}
%\phantomsection
\addcontentsline{toc}{section}{Anhang}
\addtocontents{toc}{\vspace{-0.5em}}

\end{appendix}

\end{document}